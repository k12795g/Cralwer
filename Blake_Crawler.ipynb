{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---貼上要爬蟲的頁面的第一頁，注意網址要顯示格式為....../page/，輸入為空則結束\n",
      "---"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 60>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;241m1\u001b[39m ):\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---貼上要爬蟲的頁面的第一頁，注意網址要顯示格式為....../page/，輸入為空則結束\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m---\u001b[39m\u001b[38;5;124m\"\u001b[39m,end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 61\u001b[0m     url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m url): \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---輸入爬取的網頁資料類型，食|衣|住|行|生活|育樂|醫療|養生\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m---\u001b[39m\u001b[38;5;124m\"\u001b[39m,end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mF:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py:1075\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1074\u001b[0m     )\n\u001b[1;32m-> 1075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py:1120\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1117\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1119\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m-> 1120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "#爬布雷克網站用的\n",
    "# {\n",
    "#     'Page-Title' : {\n",
    "#         'Title' : 'Page-Title',\n",
    "#         'Type' : '食|衣|住|行|生活|育樂|醫療|養生',\n",
    "#         'Author' : '網誌作者，如果有的話，沒有的話存成 無 ',\n",
    "#         'Date' : '文章發布日期',\n",
    "#         'Body' : '要用的文章，後續斷句使用',\n",
    "#         'Url' : '文章的連結'\n",
    "#     },\n",
    "# }\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "tag_list=[]\n",
    "Page_Title={}\n",
    "# Page_data={'Title':'','Type':'','Author':'無','Date' : '文章發布日期','Body' : '要用的文章，後續斷句使用','Url' : '文章的連結'}\n",
    "\n",
    "def Set_Data(url,Type,tag_list):\n",
    "    href=[]\n",
    "    Title=''\n",
    "    Author=''\n",
    "    Date=''\n",
    "    text=''\n",
    "    Number=1\n",
    "    count=0\n",
    "    while (True):\n",
    "        r=requests.get(url+str(Number))\n",
    "        Number+=1\n",
    "        soup=BeautifulSoup(r.text,\"html.parser\")\n",
    "        sel = soup.select(\"div.container div#content h2 a\") #各文章的標題連結html\n",
    "        if (not sel): break\n",
    "        for i in sel:\n",
    "            text='' #初始化合併過的內文\n",
    "            page=i.get('href')\n",
    "            page_r=requests.get(page) #開啟前面select/get到的網址\n",
    "            page_soup=BeautifulSoup(page_r.text,\"html.parser\")\n",
    "            \n",
    "            tag_check=False\n",
    "            for a in page_soup.select(\"div.post-header span.cat a\"): #若標籤存在於排除列表，則忽略此篇文章\n",
    "                if (a.text in tag_list):\n",
    "                    tag_check=True\n",
    "                    break\n",
    "            if (tag_check):continue\n",
    "                \n",
    "            Title=page_soup.select(\"div.post-header h1\")[0].text\n",
    "            Author=page_soup.select(\"div.author-content a\")[0].text\n",
    "            Date=page_soup.select(\"div.post-header span.post-date\")[0].text\n",
    "            Body_P=page_soup.select(\"div.post-entry p\")\n",
    "            for p in Body_P: #用以合併所有文本，排除圖片\n",
    "                if not p.find('img'):\n",
    "                    text+=p.text\n",
    "            Page_data={'Title':Title,'Type':Type,'Author':Author,'Date' : Date[1:],'Body' : text}\n",
    "            Page_Title[Title]=Page_data\n",
    "            \n",
    "            count+=1\n",
    "            print(\"NO.\"+str(count)+\" \"+Title)\n",
    "            \n",
    "while(1 ):\n",
    "    print(\"---貼上要爬蟲的頁面的第一頁，注意網址要顯示格式為....../page/，輸入為空則結束\\n---\",end='')\n",
    "    url=input()\n",
    "    if (not url): break\n",
    "    print(\"---輸入爬取的網頁資料類型，食|衣|住|行|生活|育樂|醫療|養生\\n---\",end='')\n",
    "    Type=input()\n",
    "    print(\"---輸入要忽略的文章tag，輸入一個後enter輸入下個，輸入為空則輸入完畢\\n---\",end='')\n",
    "    while (True):\n",
    "        tag_add=input()\n",
    "        if (tag_add):\n",
    "            tag_list.append(tag_add)\n",
    "            print(\"---\",end='')\n",
    "        else:\n",
    "            break\n",
    "    print(\"---開始爬取---\")\n",
    "    Set_Data(url,Type,tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "輸入創建的檔名，預設為'Crawl_from_Blake'\n",
      "test\n",
      "---結束---\n"
     ]
    }
   ],
   "source": [
    "print(\"輸入創建的檔名，預設為'Crawl_from_Blake'\\n\",end='')\n",
    "name=input()\n",
    "if (not name):\n",
    "    name=\"Crawl_from_Blake\"\n",
    "    print(name)\n",
    "with open(name+\".json\",\"w\",encoding='utf-8')as f:\n",
    "    json.dump(Page_Title,f,indent=4,ensure_ascii=False) #存檔\n",
    "print(\"---結束---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---貼上要爬蟲頁面的網址，且不帶有頁碼，注意網址格式為....../page/，輸入為空則結束\n",
      "---https://blake.com.tw/blog/category/taiwan/miaoli/page/\n",
      "---輸入爬取的網頁資料類型，食|衣|住|行|生活|育樂|醫療|養生\n",
      "---食\n",
      "---輸入要忽略的文章tag，可依次輸入多個，輸入為空則結束\n",
      "---新竹美食旅行\n",
      "---\n",
      "---開始爬取---\n",
      "NO.1 [美食]苗栗獅潭-阿蘭客家傳統美食-阿蘭芋糕-食尚玩家採訪過汶水老街小吃\n",
      "NO.2 [美食]苗栗南庄-日式抹茶紅豆冰-南庄必吃消熱冰品\n",
      "---貼上要爬蟲頁面的網址，且不帶有頁碼，注意網址格式為....../page/，輸入為空則結束\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "#Crawler for blake.com.tw\n",
    "# {\n",
    "#     'Page-Title' : {\n",
    "#         'Title' : 'Page-Title',\n",
    "#         'Type' : '食|衣|住|行|生活|育樂|醫療|養生',\n",
    "#         'Author' : '網誌作者，如果有的話，沒有的話存成 無 ',\n",
    "#         'Date' : '文章發布日期',\n",
    "#         'Body' : '要用的文章，後續斷句使用',\n",
    "#         'Url' : '文章的連結'\n",
    "#     },\n",
    "# }\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "tag_list=[]\n",
    "Page_Title={}\n",
    "\n",
    "def Set_Data(url,Type,tag_list):\n",
    "    href=[]\n",
    "    Title=''\n",
    "    Author=''\n",
    "    Date=''\n",
    "    text=''\n",
    "    Number=1\n",
    "    count=0\n",
    "    while (True):\n",
    "        r=requests.get(url+str(Number))\n",
    "        Number+=1\n",
    "        soup=BeautifulSoup(r.text,\"html.parser\")\n",
    "        sel = soup.select(\"div.container div#content h2 a\") #各文章的標題連結html\n",
    "        if (not sel): break\n",
    "        for i in sel:\n",
    "            text='' #初始化合併過的內文\n",
    "            page=i.get('href')\n",
    "            page_r=requests.get(page) #開啟前面select/get到的網址\n",
    "            page_soup=BeautifulSoup(page_r.text,\"html.parser\")\n",
    "            \n",
    "            tag_check=False\n",
    "            for a in page_soup.select(\"div.post-header span.cat a\"): #若標籤存在於排除列表，則忽略此篇文章\n",
    "                if (a.text in tag_list):\n",
    "                    tag_check=True\n",
    "                    break\n",
    "            if (tag_check):continue\n",
    "                \n",
    "            Title=page_soup.select(\"div.post-header h1\")[0].text\n",
    "            Author=page_soup.select(\"div.author-content a\")[0].text\n",
    "            Date=page_soup.select(\"div.post-header span.post-date\")[0].text\n",
    "            Body_P=page_soup.select(\"div.post-entry p\")\n",
    "            for p in Body_P: #用以合併所有文本，排除圖片\n",
    "                if not p.find('img'):\n",
    "                    text+=p.text\n",
    "                    \n",
    "            Page_data={'Title':Title,'Type':Type,'Author':Author,'Date' : Date[1:],'Body' : text}\n",
    "            Page_Title[Title]=Page_data\n",
    "            \n",
    "            count+=1\n",
    "            print(\"NO.\"+str(count)+\" \"+Title)\n",
    "            \n",
    "while(1 ):\n",
    "    print(\"---貼上要爬蟲頁面的網址，且不帶有頁碼，注意網址格式為....../page/，輸入為空則結束\\n---\",end='')\n",
    "    url=input()\n",
    "    if (not url): break\n",
    "    print(\"---輸入爬取的網頁資料類型，食|衣|住|行|生活|育樂|醫療|養生\\n---\",end='')\n",
    "    Type=input()\n",
    "    print(\"---輸入要忽略的文章tag，可依次輸入多個，輸入為空則結束\\n---\",end='')\n",
    "    while (True):\n",
    "        tag_add=input()\n",
    "        if (tag_add):\n",
    "            tag_list.append(tag_add)\n",
    "            print(\"---\",end='')\n",
    "        else:\n",
    "            break\n",
    "    print(\"---開始爬取---\")\n",
    "    Set_Data(url,Type,tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---輸入創建的json檔名，預設為'Crawl_from_Blake'\n",
      "苗栗_食\n",
      "---結束---\n"
     ]
    }
   ],
   "source": [
    "print(\"---輸入創建的json檔名，預設為'Crawl_from_Blake'\\n\",end='')\n",
    "name=input()\n",
    "if (not name):\n",
    "    name=\"Crawl_from_Blake\"\n",
    "    print(name)\n",
    "with open(name+\".json\",\"w\",encoding='utf-8')as f:\n",
    "    json.dump(Page_Title,f,indent=4,ensure_ascii=False) #存檔\n",
    "print(\"---結束---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

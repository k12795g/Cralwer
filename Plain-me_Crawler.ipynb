{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c5d75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import emoji\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "tag_list=[]\n",
    "Page_Title={}\n",
    "\n",
    "#tset url:https://blog.plain-me.com/page/2/\n",
    "\n",
    "def Set_Data(url,Type,tag_list):\n",
    "    href=[]\n",
    "    Title=''\n",
    "    Author=''\n",
    "    Date=''\n",
    "    text=''\n",
    "    Number=1\n",
    "    count=0\n",
    "    while (True):\n",
    "        r=requests.get(re.sub(r'page/\\d+', f'page/{Number}', url)) #用正則表達式處理網址\n",
    "        Number+=1\n",
    "        soup=BeautifulSoup(r.text,\"html.parser\")\n",
    "        sel=soup.select(\"body.home.blog div.fusion-row div.fusion-post-wrapper div.fusion-alignleft a\")\n",
    "        if (not sel): break\n",
    "        for i in sel:\n",
    "            text='' #初始化合併過的內文\n",
    "            page=i.get('href')\n",
    "            page_r=requests.get(page) #開啟前面select/get到的網址\n",
    "            page_soup=BeautifulSoup(page_r.text,\"html.parser\")\n",
    "\n",
    "            #由於存在沒有標題的文章，導致IndexError，直接將空標題文章跳過。\n",
    "            try:\n",
    "                Title=page_soup.select(\"body.post-template-default div.fusion-wrapper div.fusion-page-title-captions h1\")[0].text\n",
    "            except IndexError:\n",
    "                continue\n",
    "\n",
    "            Author=page_soup.select(\"body.post-template-default div.fusion-row span.fn a\")[0].text\n",
    "            Date=page_soup.select(\"body.post-template-default div.fusion-row div.fusion-meta-info span:not([class])\")[0].text\n",
    "            Date=datetime.strptime(Date, \"%d %m 月, %Y\")\n",
    "            Date= Date.strftime(\"%Y-%m-%d\")\n",
    "            text=page_soup.select('div.post-content')[0].text\n",
    "            text=re.sub(r'[^\\u4e00-\\u9fff，。、；：「」『』（）《》？！\\s]', '', text) #去除非中文常用之特殊符號\n",
    "            text=re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',text)\n",
    "            Page_data={'Title':Title,'Type':Type,'Author':Author,'Date' : Date,'Body' : text,'Url' : page}\n",
    "            Page_Title[Title]=Page_data\n",
    "\n",
    "            count+=1\n",
    "            print(\"NO.\"+str(count)+\" \"+Title)\n",
    "\n",
    "while(1):\n",
    "    print(\"---貼上要爬蟲頁面的網址，要帶有頁碼且頁碼隨意，但只會自動從第一頁開始，輸入為空則結束\\n---\",end='')\n",
    "    url=input()\n",
    "#     url='https://blog.plain-me.com/page/2/'\n",
    "    if (not url): break\n",
    "    print(\"---輸入爬取的網頁資料類型，食|衣|住|行|生活|育樂|醫療|養生\\n---\",end='')\n",
    "    Type=input()\n",
    "    print(\"---輸入要忽略的文章tag，可依次輸入多個，輸入為空則結束\\n---\",end='')\n",
    "    while (True):\n",
    "        tag_add=input()\n",
    "        if (tag_add):\n",
    "            tag_list.append(tag_add)\n",
    "            print(\"---\",end='')\n",
    "        else:\n",
    "            break\n",
    "    print(\"---開始爬取---\")\n",
    "    Set_Data(url,Type,tag_list)\n",
    "    \n",
    "print(\"---輸入創建的json檔名，預設為'Crawled_from_Plain-me'\\n\",end='')\n",
    "name=input()\n",
    "if (not name):\n",
    "    name=\"Crawled_from_Plain-me\"\n",
    "    print(name)\n",
    "with open(name+\".json\",\"w\",encoding='utf-8')as f:\n",
    "    json.dump(Page_Title,f,indent=4,ensure_ascii=False) #存檔\n",
    "print(\"---結束---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

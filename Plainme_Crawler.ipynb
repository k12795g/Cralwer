{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45c028f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---貼上要爬蟲頁面的網址，要帶有頁碼且頁碼隨意，但只會自動從第一頁開始，輸入為空則結束\n",
      "---https://blog.plain-me.com/page/2/\n",
      "---輸入爬取的網頁資料類型，食|衣|住|行|生活|育樂|醫療|養生\n",
      "---\n",
      "---輸入要忽略的文章tag，可依次輸入多個，輸入為空則結束\n",
      "---\n",
      "---開始爬取---\n",
      "https://blog.plain-me.com/2023/06/47038/\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 61>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---開始爬取---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 76\u001b[0m     \u001b[43mSet_Data\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43mType\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtag_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---輸入創建的json檔名，預設為\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCrawl_from_yoke918\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     79\u001b[0m name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m()\n",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36mSet_Data\u001b[1;34m(url, Type, tag_list)\u001b[0m\n\u001b[0;32m     33\u001b[0m     page_soup\u001b[38;5;241m=\u001b[39mBeautifulSoup(page_r\u001b[38;5;241m.\u001b[39mtext,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m#     tag_check=False\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m#     for a in page_soup.select(\"div.post-header span.cat a\"): #若標籤存在於排除列表，則忽略此篇文章\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m#         if (a.text in tag_list):\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m#             tag_check=True\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m#             break\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m#     if (tag_check):continue\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     Title\u001b[38;5;241m=\u001b[39m\u001b[43mpage_soup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader.entry-header h1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m     43\u001b[0m     Author\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m無\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     44\u001b[0m     Date\u001b[38;5;241m=\u001b[39mpage_soup\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime.ct-meta-element-date\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "\n",
    "tag_list=[]\n",
    "Page_Title={}\n",
    "\n",
    "#tset url:https://blog.plain-me.com/page/2/\n",
    "\n",
    "def Set_Data(url,Type,tag_list):\n",
    "    href=[]\n",
    "    Title=''\n",
    "    Author=''\n",
    "    Date=''\n",
    "    text=''\n",
    "    Number=1\n",
    "    count=0\n",
    "    while (True):\n",
    "        r=requests.get(re.sub(r'page/\\d+', f'page/{Number}', url)) #用正則表達式處理網址\n",
    "        Number+=1\n",
    "        soup=BeautifulSoup(r.text,\"html.parser\")\n",
    "        sel=soup.select(\"body.home.blog div.fusion-row div.fusion-post-wrapper div.fusion-alignleft a\")\n",
    "        if (not sel): break\n",
    "        for i in sel:\n",
    "            text='' #初始化合併過的內文\n",
    "            page=i.get('href')\n",
    "            print(page)\n",
    "            page_r=requests.get(page) #開啟前面select/get到的網址\n",
    "            page_soup=BeautifulSoup(page_r.text,\"html.parser\")\n",
    "\n",
    "        #     tag_check=False\n",
    "        #     for a in page_soup.select(\"div.post-header span.cat a\"): #若標籤存在於排除列表，則忽略此篇文章\n",
    "        #         if (a.text in tag_list):\n",
    "        #             tag_check=True\n",
    "        #             break\n",
    "        #     if (tag_check):continue\n",
    "\n",
    "            Title=page_soup.select(\"header.entry-header h1\")[0].text\n",
    "            Author=\"無\"\n",
    "            Date=page_soup.select(\"time.ct-meta-element-date\")[0].text\n",
    "            Body_P=page_soup.select(\"article.post div.entry-content p\")\n",
    "            print\n",
    "            for p in Body_P: #用以合併所有文本，排除圖片\n",
    "                text+=p.text\n",
    "                text = emoji.demojize(text) #去除emoji\n",
    "                text = text.replace('\\n','') #去除換行符\n",
    "                text = re.sub(r'[^\\u4e00-\\u9fff，。、；：「」『』（）《》？！\\s]', '', text) #去除特殊符號\n",
    "                text = re.sub(r'[^。？！]*?(延伸閱讀|看更多文章請點|網路優惠|訂房及空房查詢|訂房查詢)[^。？！]*?[。？！]?', '', text) #去除廣告連結文字\n",
    "                \n",
    "            Page_data={'Title':Title,'Type':Type,'Author':Author,'Date' : Date,'Body' : text,'Url' : page}\n",
    "            Page_Title[Title]=Page_data\n",
    "\n",
    "            count+=1\n",
    "            print(\"NO.\"+str(count)+\" \"+Title)\n",
    "\n",
    "while(1):\n",
    "    print(\"---貼上要爬蟲頁面的網址，要帶有頁碼且頁碼隨意，但只會自動從第一頁開始，輸入為空則結束\\n---\",end='')\n",
    "    url=input()\n",
    "#     url='https://yoke918.com/tag/%e6%be%8e%e6%b9%96%e6%97%85%e9%81%8a%e6%99%af%e9%bb%9e/page/2/'\n",
    "    if (not url): break\n",
    "    print(\"---輸入爬取的網頁資料類型，食|衣|住|行|生活|育樂|醫療|養生\\n---\",end='')\n",
    "    Type=input()\n",
    "    print(\"---輸入要忽略的文章tag，可依次輸入多個，輸入為空則結束\\n---\",end='')\n",
    "    while (True):\n",
    "        tag_add=input()\n",
    "        if (tag_add):\n",
    "            tag_list.append(tag_add)\n",
    "            print(\"---\",end='')\n",
    "        else:\n",
    "            break\n",
    "    print(\"---開始爬取---\")\n",
    "    Set_Data(url,Type,tag_list)\n",
    "    \n",
    "print(\"---輸入創建的json檔名，預設為'Crawl_from_yoke918'\\n\",end='')\n",
    "name=input()\n",
    "if (not name):\n",
    "    name=\"Crawl_from_yoke918\"\n",
    "    print(name)\n",
    "with open(name+\".json\",\"w\",encoding='utf-8')as f:\n",
    "    json.dump(Page_Title,f,indent=4,ensure_ascii=False) #存檔\n",
    "print(\"---結束---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1bd9ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "https://blog.plain-me.com/2023/05/46982/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import emoji\n",
    "r=requests.get('https://blog.plain-me.com/')\n",
    "print (r)\n",
    "soup=BeautifulSoup(r.text,\"html.parser\")\n",
    "url=soup.select(\"body.home.blog div.fusion-row div.fusion-post-wrapper div.fusion-alignleft a\")[1].get('href')\n",
    "print(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
